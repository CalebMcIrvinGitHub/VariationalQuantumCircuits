{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of a VQC for reinforcement learning on OpenAI Gym's Frozen Lake environment\n",
    "\n",
    "Based off of https://github.com/ycchen1989/Var-QuantumCircuits-DeepRL/blob/master/Code/QML_DQN_FROZEN_LAKE.py\n",
    "\n",
    "Implemented using Qiskit and PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "import random\n",
    "from qiskit import QuantumCircuit\n",
    "from torch.autograd import Variable\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.compiler import transpile\n",
    "from qiskit import Aer\n",
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "\n",
    "    # Initialize our replay memory\n",
    "    def __init__(self, capacity):\n",
    "        self.transitions = []\n",
    "        self.capacity = capacity\n",
    "\n",
    "    # Add a transition value to our memory\n",
    "    def store_transition(self, transition):\n",
    "        if len(self.transitions) < self.capacity:\n",
    "            self.transitions.append(transition)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.transitions)\n",
    "\n",
    "    # Sample a random batch from our memory\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.transitions, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, numQubits=4, depth=1):\n",
    "        # Number of qubits used (# of wires)\n",
    "        self.numQubits = numQubits\n",
    "\n",
    "        # Number of times to apply the CNOT / rotation module\n",
    "        self.depth = depth\n",
    "\n",
    "        # Action-value function approximator\n",
    "        self.qc = QuantumCircuit(numQubits, numQubits)\n",
    "\n",
    "        # State encoding parameters\n",
    "        # thetas: parameters used to store angles to rotate in the x direction by\n",
    "        # phis: parameters used to store angles to rotate in the z direction by\n",
    "        self.thetas = [Parameter('theta_{}'.format(i)) for i in range(self.numQubits)]\n",
    "        self.phis = [Parameter('phi_{}'.format(i)) for i in range(self.numQubits)]\n",
    "\n",
    "        # Creates rotations to be used in getting outputs\n",
    "        self.alpha_rotations = [Parameter('alpha_{}'.format(i)) for i in range(self.numQubits)]\n",
    "        self.beta_rotations = [Parameter('beta_{}'.format(i)) for i in range(self.numQubits)]\n",
    "        self.gamma_rotations = [Parameter('gamma_{}'.format(i)) for i in range(self.numQubits)]\n",
    "\n",
    "        # Creates a backend to run the circuit on\n",
    "        self.backend = Aer.get_backend('qasm_simulator')\n",
    "\n",
    "        # Initialize state preparation gates\n",
    "        self.state_preparation()\n",
    "\n",
    "        # Create a layer\n",
    "        self.init_layer()\n",
    "\n",
    "        # Initialize measurement\n",
    "        self.init_measurement()\n",
    "\n",
    "    # State is a decimal value from 0 to 16\n",
    "    # Converts this decimal value to a binary list\n",
    "    def get_binary_state_encoding(self, state):\n",
    "        encoding = [int(i) for i in bin(state)[2:]]\n",
    "        while len(encoding) < self.numQubits:\n",
    "            encoding = [0] + encoding\n",
    "        return encoding\n",
    "\n",
    "    # Creates a parameterized state encoding circuit\n",
    "    def state_preparation(self):\n",
    "        \n",
    "        # Initialize circuit with params\n",
    "        for wire in range(self.numQubits):\n",
    "            self.qc.rx(np.pi * self.thetas[wire], wire)\n",
    "        \n",
    "        for wire in range(self.numQubits):\n",
    "            self.qc.rz(np.pi * self.phis[wire], wire)\n",
    "    \n",
    "    # Binds theta values and phi values to the quantum circuit\n",
    "    # state: index of the state\n",
    "    def bind_state_preparation_parameters(self, state, circuit):\n",
    "        angles = self.get_binary_state_encoding(state)\n",
    "\n",
    "        # Make sure the number of theta values and phi values are equal to the number of qubits in the circuit\n",
    "        assert len(angles) == self.numQubits\n",
    "\n",
    "        circuit = circuit.bind_parameters(dict(zip(self.thetas, angles)))\n",
    "        circuit = circuit.bind_parameters(dict(zip(self.phis, angles)))\n",
    "\n",
    "        return circuit\n",
    "    \n",
    "    # Creates a rotation layer\n",
    "    def init_layer(self):\n",
    "        # Create CNOT gates at each layer of the circuit\n",
    "        for wire in range(self.numQubits - 1):\n",
    "            self.qc.cx(wire, wire + 1)\n",
    "        \n",
    "        # Create rotations at each level of the circuit\n",
    "        for wire in range(self.numQubits):\n",
    "            self.qc.rx(self.alpha_rotations[wire], wire)\n",
    "            self.qc.ry(self.beta_rotations[wire], wire)\n",
    "            self.qc.rz(self.gamma_rotations[wire], wire)\n",
    "\n",
    "    # Binds one layer of a parameterized circuit to parameters\n",
    "    # alpha: array storing rotation values in x direction\n",
    "    # beta: array storing rotation values in y direction\n",
    "    # gamma: array storing rotation values in z direction\n",
    "    def bind_layer(self, alphas, betas, gammas, circuit):\n",
    "        # Length of alpha, beta, and gamma must be the same as the number of wires in the circuit\n",
    "        assert len(alphas) == self.numQubits                     \n",
    "        assert len(betas) == self.numQubits\n",
    "        assert len(gammas) == self.numQubits\n",
    "\n",
    "        circuit = circuit.bind_parameters(dict(zip(self.alpha_rotations, [alpha.item() for alpha in alphas])))\n",
    "        circuit = circuit.bind_parameters(dict(zip(self.beta_rotations, [beta.item() for beta in betas])))\n",
    "        circuit = circuit.bind_parameters(dict(zip(self.gamma_rotations, [gamma.item() for gamma in gammas])))\n",
    "\n",
    "        return circuit\n",
    "    \n",
    "    # Adds a measurement layer to the end of the circuit\n",
    "    def init_measurement(self):\n",
    "        for wire in range(self.numQubits):\n",
    "            self.qc.measure(wire, wire)\n",
    "\n",
    "    def run_job(self, circuit, num_iterations=10):\n",
    "        # Create a job to run on our circuit\n",
    "        job = self.backend.run(transpile(circuit, self.backend), shots=num_iterations)\n",
    "\n",
    "        # Get the result of our job\n",
    "        results = job.result()\n",
    "\n",
    "        # Get the number of times each result appeared\n",
    "        result_counts = results.get_counts(circuit)\n",
    "\n",
    "        # For each result we got, get the number of times each bit appeared\n",
    "        # The bit that was 1 most often is the index of our selected action\n",
    "        counts = [0] * self.numQubits\n",
    "        for output in result_counts.keys():\n",
    "            for wire in range(self.numQubits):\n",
    "                counts[wire] += int(output[wire]) * result_counts[output]\n",
    "\n",
    "        return counts\n",
    "\n",
    "    # Outputs a score for each action\n",
    "    # If expectation value from qubit 0 is highest, then action selected is LEFT\n",
    "    # If expectation value from qubit 1 is highest, then action selected is DOWN\n",
    "    # If expectation value from qubit 2 is highest, then action selected is RIGHT\n",
    "    # If expectation value from qubit 3 is highest, then action selected is UP\n",
    "    # state: the state to select an action from\n",
    "    # params: tuple containing alphas, betas, and gammas\n",
    "    # num_iterations: the number of times to use to calculate our expectation values\n",
    "    # epsilon: percentage of the time to choose a random action\n",
    "    def get_q_values(self, state, params, num_iterations=10, epsilon=0.1):\n",
    "        if np.random.rand() < epsilon:\n",
    "            random_q = np.random.rand(4)\n",
    "            normalized_random_q = [i / np.sum(random_q) for i in random_q]\n",
    "            return normalized_random_q\n",
    "\n",
    "        # Binds the state preparation parameters to the state we want to select the best action from\n",
    "        bound_copy = self.qc.copy()\n",
    "        bound_copy = self.bind_state_preparation_parameters(state, bound_copy)\n",
    "        bound_copy = self.bind_layer(*params, bound_copy)\n",
    "\n",
    "        counts = self.run_job(bound_copy, num_iterations)\n",
    "        \n",
    "        # Normalization\n",
    "        counts = [i / (sum(counts) if sum(counts) != 0 else 1 / len(counts)) for i in counts]\n",
    "\n",
    "        return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner():\n",
    "\n",
    "    # Initialize the runner class\n",
    "    # num_episodes: the number of episodes to run\n",
    "    # epsilon: the percentage of the time to select a random action\n",
    "    # capacity: the capacity of the replay memory\n",
    "    def __init__(self, num_episodes=100, epsilon=0.1, capacity=1000, batch_size=4):\n",
    "        self.num_episodes = num_episodes\n",
    "        self.epsilon = epsilon\n",
    "        self.memory = ReplayMemory(capacity)\n",
    "        self.agent = Agent(numQubits=4, depth=1)\n",
    "        self.target_agent = Agent(numQubits=4, depth=1)\n",
    "        self.parameters = Variable(torch.tensor(np.random.rand(3, 4) * 2 - 1).type(torch.DoubleTensor), requires_grad=True)\n",
    "        self.target_parameters = self.parameters.clone().detach()\n",
    "        self.env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "        self.batch_size = batch_size\n",
    "        self.terminal_state = 15\n",
    "        self.opt = torch.optim.RMSprop([self.parameters], lr=0.01, alpha=0.99, eps=1e-08)\n",
    "        self.gamma = 0.1\n",
    "        self.steps = 0\n",
    "        self.update_target_every_n_steps = 10\n",
    "        self.labels = torch.tensor([])\n",
    "        self.cumulative_rewards = []\n",
    "        \n",
    "    def criterion(self, y_pred, y):\n",
    "        return torch.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    def backwards(self):\n",
    "        self.opt.zero_grad()\n",
    "        loss = self.criterion(self.labels, self.parameters)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    # Run the algorithm for num_episodes\n",
    "    # num_episodes: the number of episodes to run (M)\n",
    "    def run(self):\n",
    "        # Initialize random parameters\n",
    "        for ep in tqdm(range(self.num_episodes)):\n",
    "            initial_state = self.env.reset()[0]\n",
    "            prev_state = initial_state\n",
    "            terminated, truncated = False, False\n",
    "            cumulative_ep_reward = 0\n",
    "            while not terminated and not truncated:\n",
    "\n",
    "                self.steps += 1\n",
    "                \n",
    "                # Use our quantum function approximator to get an action\n",
    "                # alphas, betas, gammas, = self.parameters\n",
    "                # action = np.argmax(self.agent.get_q_values(initial_state, (alphas, betas, gammas)))\n",
    "                action = np.argmax(self.agent.get_q_values(initial_state, self.parameters))\n",
    "\n",
    "                # Take selected action\n",
    "                observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "                \n",
    "                # Reward shaping to penalize for falling in a hole\n",
    "                if terminated or truncated and observation != self.terminal_state:\n",
    "                    reward -= 1\n",
    "\n",
    "                # Update cumulative reward\n",
    "                cumulative_ep_reward += reward\n",
    "\n",
    "                # Store experience\n",
    "                self.memory.store_transition((prev_state, action, reward, observation))\n",
    "                prev_state = observation\n",
    "\n",
    "                # If we have very little experience built up, continue\n",
    "                if self.memory.size() < self.batch_size:\n",
    "                    continue\n",
    "                \n",
    "                # Get minibatch of transitions\n",
    "                transitions = self.memory.sample(self.batch_size)\n",
    "\n",
    "                # Get labels for the transitions\n",
    "                labels = []\n",
    "                for transition in transitions:\n",
    "                    label = transition[2]\n",
    "                    if transition[3] != self.terminal_state:\n",
    "                        # target_alphas, target_betas, target_gammas = self.target_parameters\n",
    "                        # label += self.gamma * np.max(self.target_agent.get_q_values(initial_state, (target_alphas, target_betas, target_gammas)))\n",
    "                        label += self.gamma * np.max(self.target_agent.get_q_values(initial_state, self.target_parameters))\n",
    "                    labels.append(label)\n",
    "\n",
    "                self.labels = torch.tensor(labels)\n",
    "                \n",
    "                # Backwards step\n",
    "                self.opt.step(self.backwards)\n",
    "\n",
    "                # Update target parameters\n",
    "                if self.steps % self.update_target_every_n_steps == 0:\n",
    "                    self.target_parameters = self.parameters.clone().detach()\n",
    "\n",
    "            self.cumulative_rewards.append(cumulative_ep_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [04:05<13:40, 10.65s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24392/980417278.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRunner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# r.agent.get_q_values(0, ([0.1, 0.6, 0.3, 0.4], [-0.2, 0.1, 0.3, -0.5], [0.1, 0.1, 0.3, -0.2]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24392/3689789847.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     48\u001b[0m                 \u001b[1;31m# alphas, betas, gammas, = self.parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[1;31m# action = np.argmax(self.agent.get_q_values(initial_state, (alphas, betas, gammas)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[1;31m# Take selected action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24392/629614256.py\u001b[0m in \u001b[0;36mget_q_values\u001b[1;34m(self, state, params, num_iterations, epsilon)\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mbound_copy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[0mbound_copy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind_state_preparation_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbound_copy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[0mbound_copy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbound_copy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mcounts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_job\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbound_copy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Caleb\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    646\u001b[0m                           \u001b[1;34m'iterations executed (and might lead to errors or silently give '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m                           'incorrect results).', category=torch.jit.TracerWarning, stacklevel=2)\n\u001b[1;32m--> 648\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "r = Runner()\n",
    "r.run()\n",
    "# r.agent.get_q_values(0, ([0.1, 0.6, 0.3, 0.4], [-0.2, 0.1, 0.3, -0.5], [0.1, 0.1, 0.3, -0.2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.cumulative_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, {'prob': 1})\n",
      "(4, 0.0, False, False, {'prob': 1.0})\n",
      "(5, 0.0, True, False, {'prob': 1.0})\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11596/3967009373.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "print(env.reset())\n",
    "while True:\n",
    "    action = int(input())\n",
    "    print(env.step(action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b71e2ea7fd88c01752403482f8b390a2bb97379ffce9aede2d7f28ae0381b030"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
